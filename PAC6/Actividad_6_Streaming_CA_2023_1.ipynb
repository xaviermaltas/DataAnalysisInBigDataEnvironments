{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ecb0e",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"XAVIER MALTAS TARRIDAS\"\n",
    "COLLABORATORS = \"OSCAR BUISAN VINIEGRA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789edc61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44302dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9d3c99fc80dcffd63404a4e64894569",
     "grade": false,
     "grade_id": "cell-a1f498c5f499f12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PAC 6 - Introducció a Spark Streaming i Structured Streaming\n",
    "\n",
    "## Introducció\n",
    "A l'activitat anterior, vas aprendre sobre dues eines populars per adquirir dades en temps real: Flume i Kafka. En aquesta activitat, treballarem amb un conjunt d'eines de streaming de Spark i aprendràs com processar dades en temps real utilitzant Spark Streaming. Exploraràs el poder de Spark Streaming i Structured Streaming, dues eines potents disponibles a la biblioteca Spark.\n",
    "\n",
    "### [Spark Streaming](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html)\n",
    "Spark Streaming és un motor de processament d'streams escalable i tolerant a fallades construït sobre Apache Spark. Permet el processament de dades en temps real amb un alt rendiment i baixa latència. Amb Spark Streaming, pots realitzar analítiques en temps real, aprenentatge automàtic i processament de gràfics sobre dades en streaming.\n",
    "\n",
    "### [Spark Structured Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html)\n",
    "Structured Streaming és una API de nivell alt per al processament d'streams a Spark. Proporciona una interfície declarativa i similar a SQL per al processament de dades estructurades en streaming. Amb Structured Streaming, pots escriure consultes en temps real que s'integren fàcilment amb el processament per lots, permetent-te construir pipelines de processament de dades end-to-end.\n",
    "\n",
    "## Objectius\n",
    "En els diferents exercicis pràctics, aprendràs a:\n",
    "- Configurar una aplicació de Spark Streaming\n",
    "- Processar i analitzar dades en temps real\n",
    "- Construir pipelines de processament de dades de punta a punta\n",
    "\n",
    "## Començar\n",
    "L'activitat es basarà en la xarxa social Mastodon per processar les seves dades en streaming. És una bona idea conèixer l'estructura JSON d'un 'toot' - consulta el [web de l'API de Mastodon](https://docs.joinmastodon.org/entities/Status/) per facilitar la comprensió dels diferents exercicis que proposem.\n",
    "\n",
    "A més, has de conèixer [Spark Streaming](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html) i [Spark Structured Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html). A més de treballar amb els materials que compartim al campus, és interasant llegir la documentació oficial de la versió 2.4.0, que és la que està instal·lada al servidor.\n",
    "\n",
    "## Notes Importants\n",
    "- L'activitat s'ha de realitzar en **grups de 2 membres**. Assegura't de conèixer qui és el teu company abans de començar l'activitat.\n",
    "- Tot i que és possible completar les activitats en aquest quadern, **desaconsellem fer-ho** degut a possibles problemes de rendiment del servidor. Veuràs que cada activitat està autocontinguda dins de la seva cel·la, permetent-te copiar-la fàcilment en un fitxer de Python. Aquest fitxer es pot executar al servidor mitjançant SSH o VSCode. Després d'haver executat i provat l'script amb èxit, simplement **copieu-lo de nou a la cel·la corresponent del quadern**. Aquest enfocament garanteix una execució més suau i una millor gestió dels recursos del servidor.\n",
    "- **Només has d'utilitzar les llibreries proporcionades, llevat que s'indiqui explícitament el contrari.**\n",
    "- Si us plau, no canvieu el nom del quadern ni els tipus de cel·la."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1094e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83fafe8a867b57b0801701a7abc1a832",
     "grade": false,
     "grade_id": "activity_1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Activitat 1: Consumidor de Tòpics de Kafka (0.5 punt)\n",
    "\n",
    "L'objectiu d'aquest primer exercici és **crear un script que es connecti a un tòpic de Kafka i imprimeixi el seu contingut** a la terminal. Per completar l'exercici, us proporcionem un tòpic Kafka (`mastodon_toots`) que distribueix el contingut de l'stream principal de Mastodon.\n",
    "\n",
    "**Passos per Completar l'Activitat:**\n",
    "1. Configura un consumidor de Kafka per connectar-te al tòpic `mastodon_toots`.\n",
    "2. Consumix els missatges del tema.\n",
    "3. Imprimeix el contingut del camp (si existeix) de cada toot a la terminal.\n",
    "\n",
    "Tingues en compte que els toots **s'emmagatzemen com a `JSON`**, així que has de garantir una conversió adequada de les dades per treballar amb elles. A més, tingues en compte que proporcionem una funció, `extract_text_from_html`. Pots utilitzar aquesta funció auxiliar per *renderitzar* el contingut del toot, que està codificat en HTML.\n",
    "\n",
    "Comencem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1_kafkaConsumer.py\n",
    "# python3 a1_kafkaConsumer.py\n",
    "\n",
    "# Create a consumer that subscribes to the Kafka topic and digest the toots\n",
    "from tokenize import group\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Extract the text from the HTML content\n",
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    return text\n",
    "\n",
    "# Kafka Configuration\n",
    "# kafka_server = 'Cloudera02:9092'  # Kafka server address\n",
    "kafka_server = ['Cloudera02:9092', 'Cloudera03:9092']  # Kafka server address\n",
    "kafka_topic = 'mastodon_toots'   # Kafka topic\n",
    "kafka_group = 'xmaltast'   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    kafka_topic,\n",
    "    bootstrap_servers=kafka_server,\n",
    "    group_id=kafka_group,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Print the toots to the console\n",
    "try:\n",
    "    print(\"Streaming started.\")\n",
    "    for message in consumer:\n",
    "        # Convert the message to a JSON object\n",
    "        toot = message.value\n",
    "\n",
    "        # Check if 'content' field exists in the toot\n",
    "        if 'content' in toot:\n",
    "            # Extract text from HTML content\n",
    "            toot_text = extract_text_from_html(toot['content'])\n",
    "            print(\"Toot Content:\", toot_text)\n",
    "except KeyboardInterrupt:\n",
    "    # Close the consumer\n",
    "    print(\"Streaming stopped.\")\n",
    "    consumer.close()\n",
    "\n",
    "consumer.close()\n",
    "#END <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e718b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee938f70a8c27c4662cf8f35f99fd74",
     "grade": false,
     "grade_id": "cell-7463c5cedd1e61de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Activitat 2: Spark Streaming\n",
    "En aquest exercici, analitzaràs l'activitat a Mastodon comptant els toots en una finestra de temps. Aquests tipus d'estadístics son una de les operacions fonamentals a Spark, i en aquesta activitat, farem servir la biblioteca Spark Streaming per analitzar les dades en aquest aspecte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7d94c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc958688ff31e111615b3126db28cd0b",
     "grade": false,
     "grade_id": "activity_2_1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 2.1: Comptar en Finestres (0.5 punt)\n",
    "\n",
    "Com potser sabràs, la biblioteca Spark Streaming **processa dades utilitzant el concepte de finestres temporals**, agrupant elements de dades basant-se en el moment en què van ser rebuts. Aquest enfocament permet el processament per lots de dades en streaming, possibilitant anàlisis **en intervals de temps diferents**. Trobaràs que la sintaxi per realitzar operacions en RDDs dins d'aquestes finestres temporals és **pràcticament equivalent a les operacions estàndard de RDD** que ja coneixes.\n",
    "\n",
    "Completa el codi següent per obtenir el **nombre de toots originals publicats cada cinc segons**. Exclou els retuits del teu recompte. Potser necessitaràs consultar la [API de Mastodon](https://docs.joinmastodon.org/entities/Status/) per entendre com estan estructurats els toots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a21_comptarFinestres.py\n",
    "# python3 a21_comptarFinestres.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"TootCount\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the StreamingContext\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = 'Cloudera02:9092,Cloudera03:9092'  # Kafka server address\n",
    "kafka_topic = \"mastodon_toots\"   # Kafka topic\n",
    "kafka_group = \"xmaltast\"   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "} \n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafkaParams)\n",
    "\n",
    "# Count each toot as 1 and update the total count excluding retweets\n",
    "tootCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1]))\\\n",
    "    .filter(lambda toot: 'retweeted_status' not in toot) \\\n",
    "    .map(lambda x: (\"toot\", 1))\\\n",
    "    .updateStateByKey(lambda values, total: sum(values) + (total or 0))\n",
    "\n",
    "# Print the cumulative count every 5 seconds\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8377bb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e6f7e9a63397d2d2340002d1d6cee8e",
     "grade": false,
     "grade_id": "activity_2_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 2.2: Comptar Toots i agrupar per idioma (1 punt)\n",
    "\n",
    "Com has observat a l'Exercici 2.1, el procés és bastant similar a treballar amb RDDs. Ara, aprofundirem en un anàlisi més complex **comptant el nombre de toots originals per idioma es creen cada 5 segons.** Per a una millor llegibilitat, us demanem que ordeneu els idiomes en ordre descendent segons el nombre de toots i limiteu la sortida als 10 primers idiomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a32_agregacioDadesFlux.py\n",
    "# python3 a32_agregacioDadesFlux.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, count\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_2_xmaltast\"  # Replace with your Spark app name, must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = 'mastodon_toots'\n",
    "kafka_bootstrap_servers = 'Cloudera02:9092,Cloudera03:9092'  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", '{\"' + kafka_topic + '\":{\"0\": -1}}') \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the inferred schema. Then select the columns we need.\n",
    "toots_df = toots \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .filter(\"parsed_value is not null and parsed_value.language is not null\") \\\n",
    "    .select(\"parsed_value.language\") \\\n",
    "    .groupBy(\"language\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(col(\"count\").desc())  # Order by the count in descending order\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449f5d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae756eaae01b5c372c28c3b0ac94266a",
     "grade": false,
     "grade_id": "activity_2_3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 2.3: Acumular els comptatges (1 punt)\n",
    "\n",
    "Fins ara, hem obtingut resultats específics de l'stream dels micro-lots generats per finestra, que són generalment útils. Però què passa si volem mantenint la informació a través de finestres per, per exemple, acumular tendències amb el temps? En aquest exercici treballarem aquest concepte.\n",
    "\n",
    "Et convidem a ajustar l'script anterior per **mantenir un recompte de tots els toots originals, categoritzats per idioma**. En lloc de simplement comptar nous toots cada cinc segons, **els anirem afegint de manera contínua**. Pensa-hi com en una puntuació que s'actualitza constantment amb el nombre total de toots originals en cada idioma des del moment en què comencem la transmissió.\n",
    "\n",
    "Per aconseguir això, jugarem amb les [**transformacions stateful d'Spark Streaming**](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#caching--persistence). Aquesta és una una manera sofisticada de dir que recordarem dades anteriors i les utilitzarem en els nostres càlculs actuals. És semblant a mantenir un total en curs en una variable global en lloc de començar des de zero cada vegada.\n",
    "\n",
    "***Et convidem a completar l'script següent:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a32_agregacioDadesFlux.py\n",
    "# python3 a32_agregacioDadesFlux.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, count\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_2_xmaltast\"  # Replace with your Spark app name, must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = 'mastodon_toots'\n",
    "kafka_bootstrap_servers = 'Cloudera02:9092,Cloudera03:9092'  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", '{\"' + kafka_topic + '\":{\"0\": -1}}') \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the inferred schema. Then select the columns we need.\n",
    "toots_df = toots \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .filter(\"parsed_value is not null and parsed_value.language is not null\") \\\n",
    "    .select(\"parsed_value.language\") \\\n",
    "    .groupBy(\"language\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(col(\"count\").desc())  # Order by the count in descending order\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f9e79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7fdc61cc0b8bf0104b35f9b376c413",
     "grade": false,
     "grade_id": "activity_2_4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 2.4: Comptatge amb Finestres Temporals (1 punt)\n",
    "\n",
    "Com has observat, Spark Streaming és increïblement flexible i fàcil d'utilitzar, i aquí tens un truc interessant que pot fer: **et permet trobar un punt òptim entre comptar toots per [finestra de temps](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#window-operations) i mantenir un recompte en curs.** Imaginem que volem crear un tauler de control, com un panell de control, que mostri el nombre de toots fets en cada idioma. La particularitat és que **volem aquesta actualització cada 5 segons, però estem seguiment els comptatges al llarg d'un minut complet.**\n",
    "\n",
    "Així doncs, cada 5 segons, el nostre tauler de control es refresca, proporcionant-nos el darrer recompte d'un minut. És com tenir una puntuació en directe que s'actualitza amb freqüència i també fa un seguiment del que ha passat en els últims 60 segons, no només en els últims 5. D'aquesta manera, obtens tant actualitzacions immediates com una visió més amplia del que està passant, tot al mateix temps. Mostra només els 10 primers idiomes.\n",
    "\n",
    "***Modifica l'script següent per assolir aquest objectiu:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd878fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a24_comptarFinestresTemporals.py\n",
    "# python3 a24_comptarFinestresTemporals.py\n",
    "# This code provides counts within time-based windows, offering more insights into the distribution of toots over specified intervals. \n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"TopLanguagesWindowedCounts\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "window_duration = 60  # Window duration in seconds\n",
    "slide_duration = 5  # Slide duration in seconds\n",
    "\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = 'Cloudera02:9092,Cloudera03:9092'  # Kafka server address\n",
    "kafka_topic = 'mastodon_toots'   # Kafka topic\n",
    "kafka_group = 'xmaltast'   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "} \n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafkaParams)\n",
    "\n",
    "# Count each toot as 1 and update the total count. Use a 60-second window with a 5-second slide\n",
    "tootCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1]))\\\n",
    "    .filter(lambda toot: \"language\" in toot and toot[\"language\"] is not None)\\\n",
    "    .map(lambda toot: (toot[\"language\"], 1))\\\n",
    "    .updateStateByKey(lambda new_values, running_count: sum(new_values) + (running_count or 0))\\\n",
    "    .window(windowDuration=window_duration, slideDuration=slide_duration)\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48259f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e891e7423b5831493f3e6707ab78abd",
     "grade": false,
     "grade_id": "activity_2_5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 2.5: Potenciant (1 punt)\n",
    "\n",
    "D'acord, ja sabem que els RDDs a Spark són increïblement versàtils: pots fer pràcticament qualsevol operació amb ells. No obstant això, **a mesura que les coses es fan més complexes, el repte augmenta**.\n",
    "\n",
    "Ara bé, donem-li més força al nostre tauler de control. En lloc de simplement mostrar el nombre de toots per minut, **afegim algunes característiques noves i interessants**. No seria interessant **fer un seguiment de la longitud mitjana d'aquests toots?** I encara millor: **descobrim qui és l'usuari més seguit entre els toots publicats en aquest minut**.\n",
    "\n",
    "Però espera, **encara hi ha més!** Per fer tota aquesta informació molt fàcil de llegir i entendre, la **presentarem en** un format de **taula neta**. No només es tracta de les dades, sinó de fer-ho fàcil d'usar i visualment digestible.\n",
    "\n",
    "La taula resultant s'ha d'actualitzar en intervals de 5 segons, i les finestres mitjanes han de ser de 60 segons. Les columnes d'aquesta taula han de ser:\n",
    "- **lang:** Idioma\n",
    "- **num_toots:** Nombre de toots originals en aquest idioma\n",
    "- **avg_len_content:** Longitud mitjana (en caràcters) del contingut del toot\n",
    "- **user:** Usuari més seguit\n",
    "- **followers:** Nombre de seguidors d'aquest usuari\n",
    "\n",
    "Per fer la sortida més llegible, limita el nombre de files a 10.\n",
    "\n",
    "**SUGGERIMENT:** Hi ha un exemple molt útil a la [Guia de Programació en Streaming de Spark](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#dataframe-and-sql-operations). Busca'l!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00596d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92814000ee103510353f06b28f2dc0e6",
     "grade": false,
     "grade_id": "cell-9d6ae947ee64ede2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Activitat 3: Structured Streaming\n",
    "\n",
    "Com has vist a l'últim exercici, depenent de les operacions, l'API de Spark Streaming pot no ser tan convenient, especialment perquè has de treballar amb APIs de baix nivell. Afortunadament, **Spark proporciona una API de nivell alt anomenada [Spark Structured Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html)** que et permet **realitzar càlculs en streaming de la mateixa manera que expressaries un càlcul en lot sobre dades estructurades estàtiques**; com les que pots utilitzar en processament per lots.\n",
    "\n",
    "En aquest conjunt d'exercicis, et submergiràs en el fascinant món de Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5775270",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "306c9f98ac1211c6fe3b1b6e8e8c781a",
     "grade": false,
     "grade_id": "activity_3_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 3.1: Obtenir l'Esquema (1 punt)\n",
    "\n",
    "Una de les coses més interessants sobre Spark Structured Streaming és **com gestiona les dades estructurades**. Per exemple, el flux de dades al nostre tòpic de Kafka, on **cada toot arriba en un format JSON ordenat**.\n",
    "\n",
    "Similar a treballar amb els DataFrames de Spark, **Structured Streaming utilitza esquemes de dades per analitzar i formatejar aquestes dades estructurades**. En el processament per lots, Spark sovint pot deduir aquesta estructura directament de les dades. No obstant això, amb les dades en streaming, és una mica diferent: **necessitem definir aquesta estructura prèviament**.\n",
    "\n",
    "En els exercicis següents, farem servir un truc convenient: **enlloc de definir manualment** l'estructura complexa d'un toot, inicialment **extreurem alguns toots de Kafka i els analitzarem en lot per aprendre el seu esquema**. És com fer una ullada per entendre com estan organitzades les coses. Un cop tinguem l'esquema, l'aplicarem al nostre pipeline de streaming.\n",
    "\n",
    "La teva tasca en aquest exercici és fer que aquesta transformació es realitzi. Després, fent servir les operacions de DataFrame amb les quals ja estàs familiaritzat, es demana crear una taula amb les següents columnes que ens permetran veure els toots individualment a mesura que són digerits:\n",
    "- **id:** Identificador únic per a cada toot.\n",
    "- **created_at:** Marca de temps quan es va crear el toot.\n",
    "- **content:** Què diu realment el toot.\n",
    "- **language:** Idioma del toot.\n",
    "- **username:** Maneta de l'autor del toot.\n",
    "- **followers_count:** Nombre de seguidors que té l'autor.\n",
    "\n",
    "Recorda que **ens interessen els toots originals!** Filtra aquells que corresponguin a retuits.\n",
    "\n",
    "Un altre aspecte fonamental que has de gestionar aquí és **seleccionar el mode de sortida adequat**! Mira la [documentació](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#output-modes) i tria el que millor s'adapti a aquest exercici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d050ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a31_obtenirEsquema.py\n",
    "# python3 a31_obtenirEsquema.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_1_xmaltast\"  # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = 'mastodon_toots'\n",
    "kafka_bootstrap_servers = 'Cloudera02:9092,Cloudera03:9092'  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Determine the ending offset dynamically based on the available data\n",
    "ending_offset = batch_df.selectExpr(\"max(CAST(offset AS LONG)) as max_offset\").collect()[0][\"max_offset\"]\n",
    "\n",
    "# Use the dynamically determined ending offset for the batch read\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":\" + str(ending_offset) + \"}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the inferred schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .select(\n",
    "        col(\"parsed_value.id\").alias(\"id\"),\n",
    "        col(\"parsed_value.created_at\").alias(\"created_at\"),\n",
    "        col(\"parsed_value.content\").alias(\"content\"),\n",
    "        col(\"parsed_value.language\").alias(\"language\"),\n",
    "        col(\"parsed_value.account.username\").alias(\"USERNAME\"),\n",
    "        col(\"parsed_value.account.followers_count\").alias(\"followers_count\")\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"console\")\\\n",
    "            .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff5fe6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07a8183e99dc9a82514809408e0513dd",
     "grade": false,
     "grade_id": "activity_3_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 3.2: Agregació de Dades d'un Flux (1 punt)\n",
    "\n",
    "Spark Structured Streaming és realment potent, especialment quan realitzes operacions sobre un flux continu de dades. En aquest exercici, ens aprofundirem en Structured Spark Streaming, **centrant-nos específicament en l'agregació de dades d'un flux de Kafka**. És semblant al que hem fet a l'Exercici 2. La teva missió és **comptar el nombre de toots originals en cada idioma**.\n",
    "\n",
    "Així és com ha de ser la **sortida:**\n",
    "\n",
    "- **language:** Aquesta columna mostra l'idioma dels toots.\n",
    "- **count:** Aquí és on mostraràs el nombre de toots per a cada idioma.\n",
    "\n",
    "La teva taula ha **d'acumular continuament aquestes comptatges cada 10 segons**. A més, per facilitar al visualizació us demanem **ordenar els idiomes pel nombre de toots, amb els idiomes més parlants a la part superior**.\n",
    "\n",
    "Ara bé, aquí tens una **part clau** d'aquest exercici: **has de triar el mode de sortida adequat** per a la teva consulta en streaming. Recorda, **el mode de sortida determina com es escriu cada lot resultant de dades en l'origen de sortida**. Les teves opcions són 'Complete', 'Append' i 'Update'. Pensa en quin encaixa millor amb el nostre escenari de comptatge acumulatiu i ordenat. I **no oblidis escriure la teva raonament en forma de comentari al codi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a32_agregacioDadesFlux.py\n",
    "# python3 a32_agregacioDadesFlux.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, count\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_2_xmaltast\"  # Replace with your Spark app name, must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = 'mastodon_toots'\n",
    "kafka_bootstrap_servers = 'Cloudera02:9092,Cloudera03:9092'  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", '{\"' + kafka_topic + '\":{\"0\": -1}}') \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the inferred schema. Then select the columns we need.\n",
    "toots_df = toots \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .filter(\"parsed_value is not null and parsed_value.language is not null\") \\\n",
    "    .select(\"parsed_value.language\") \\\n",
    "    .groupBy(\"language\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(col(\"count\").desc())  # Order by the count in descending order\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747492dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d6f4423d0479da3745b3cc26e5d305b",
     "grade": false,
     "grade_id": "activity_3_3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 3.3: Comptatge per Finestres (1 punt)\n",
    "\n",
    "Bona feina! Has après com realitzar agregacions i fer un seguiment dels comptatges al llarg del temps. Com has vist a l'Exercici 2.4, a vegades és més efectiu mantenir aquests **comptatges dins de finestres de temps específiques**. Ara volem que apliquis aquesta tècnica utilitzant les [funcions disponibles a Spark Structured Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#window-operations-on-event-time). Tingues en compte que Spark Structured Streaming **gestiona el temps de manera diferent que Spark Streaming**, tingueu en compte a l'hora d'analitzar i interpretar els resultats.\n",
    "\n",
    "La teva tasca és **crear una taula que mostri el comptatge del nombre de toots originals (recorda filtrar els retuits) fets en cada idioma, segmentats dins d'un marc de temps específic**. Per a aquest exercici, has de fer servir una finestra lliscant d'un minut, amb les dades actualitzant-se cada 5 segons. Aquest enfocament et permetrà monitorar de prop la freqüència dels toots en diferents idiomes durant intervals breus i superposats.\n",
    "\n",
    "Et demanem que proporcionis una **taula amb la següent estructura**:\n",
    "- **window:** Mostra l'interval de temps.\n",
    "- **language:** Aquesta columna mostra l'idioma dels toots.\n",
    "- **count:** Aquí és on mostraràs el nombre de toots per a cada idioma.\n",
    "\n",
    "Els resultats han d'estar **ordenats per finestra de temps i count en ordre descendent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df5fd4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'No such struct field is_reblog in account, activity_pub_type, akkoma, all_emojis, application, bookmarked, card, communities, content, content_type, conversation_id, created_at, edited_at, emoji_reactions, emoji_reactions_count, emojis, favourited, favourites_count, filtered, friendica, id, in_reply_to_account_id, in_reply_to_id, in_reply_to_status, is_meta_preview, is_only_for_followers, is_rss_content, iscn_id, language, limited, limited_scope, local_only, markdown, media_attachments, mentions, meta_title, muted, nyaize_content, pinned, plain_content, pleroma, poll, processing, profile_emojis, quote, quote_id, reactions, reactions_count, reblog, reblogged, reblogs_count, replies_count, rss_host_url, rss_link, searchability, sensitive, spoiler_text, status_reference_ids, status_references_count, status_referred_by_count, support_likers, tags, text, text_count, translated_text, updated_at, uri, url, visibility, visibility_ex; line 1 pos 71'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.filter.\n: org.apache.spark.sql.AnalysisException: No such struct field is_reblog in account, activity_pub_type, akkoma, all_emojis, application, bookmarked, card, communities, content, content_type, conversation_id, created_at, edited_at, emoji_reactions, emoji_reactions_count, emojis, favourited, favourites_count, filtered, friendica, id, in_reply_to_account_id, in_reply_to_id, in_reply_to_status, is_meta_preview, is_only_for_followers, is_rss_content, iscn_id, language, limited, limited_scope, local_only, markdown, media_attachments, mentions, meta_title, muted, nyaize_content, pinned, plain_content, pleroma, poll, processing, profile_emojis, quote, quote_id, reactions, reactions_count, reblog, reblogged, reblogs_count, replies_count, rss_host_url, rss_link, searchability, sensitive, spoiler_text, status_reference_ids, status_references_count, status_referred_by_count, support_likers, tags, text, text_count, translated_text, updated_at, uri, url, visibility, visibility_ex; line 1 pos 71\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.findField(complexTypeExtractors.scala:85)\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:53)\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq$$anonfun$7.apply(package.scala:244)\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq$$anonfun$7.apply(package.scala:243)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:243)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$39.apply(Analyzer.scala:889)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$39.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:888)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:900)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:900)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:758)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1484)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1498)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e0b64fa40735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mtoots_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoots\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parsed_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parsed_value is not null and parsed_value.language is not null and not parsed_value.is_reblog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parsed_value.language\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     .groupBy(\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m   1357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'No such struct field is_reblog in account, activity_pub_type, akkoma, all_emojis, application, bookmarked, card, communities, content, content_type, conversation_id, created_at, edited_at, emoji_reactions, emoji_reactions_count, emojis, favourited, favourites_count, filtered, friendica, id, in_reply_to_account_id, in_reply_to_id, in_reply_to_status, is_meta_preview, is_only_for_followers, is_rss_content, iscn_id, language, limited, limited_scope, local_only, markdown, media_attachments, mentions, meta_title, muted, nyaize_content, pinned, plain_content, pleroma, poll, processing, profile_emojis, quote, quote_id, reactions, reactions_count, reblog, reblogged, reblogs_count, replies_count, rss_host_url, rss_link, searchability, sensitive, spoiler_text, status_reference_ids, status_references_count, status_referred_by_count, support_likers, tags, text, text_count, translated_text, updated_at, uri, url, visibility, visibility_ex; line 1 pos 71'"
     ]
    }
   ],
   "source": [
    "# a33_comptatgeFinestres.py\n",
    "# python3 a33_comptatgeFinestres.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col, window, count\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_3_xmaltast\"  # Replace with your Spark app name, must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = 'mastodon_toots'\n",
    "kafka_bootstrap_servers = 'Cloudera02:9092,Cloudera03:9092'  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the inferred schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "\n",
    "# Print the schema to understand the structure\n",
    "toots_df.printSchema()\n",
    "\n",
    "# Filter based on the correct nested field\n",
    "filtered_toots_df = toots_df \\\n",
    "    .filter(col(\"parsed_value.quote.account.display_name\") == \"some_value\") \\\n",
    "    .select(\n",
    "        \"parsed_value.text\",            # Replace with the actual text field\n",
    "        \"parsed_value.created_at\",      # Replace with the actual created_at field\n",
    "        \"parsed_value.group_by_column\"  # Replace with the actual group by column\n",
    "    ) \\\n",
    "    .groupBy(\n",
    "        window(\"parsed_value.created_at\", \"5 minutes\"),\n",
    "        \"parsed_value.group_by_column\"\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window.start\", \"window.end\", ascending=False)\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = filtered_toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"complete\")\\\n",
    "            .format(\"console\")\\\n",
    "            .option(\"truncate\", \"false\")\\\n",
    "            .trigger(processingTime='5 seconds')\\\n",
    "            .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831a26a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c5a6d63b1eb350a11f37b62eee41214",
     "grade": false,
     "grade_id": "activity_3_4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Activitat 3.4: Unió de Fluxos (1 punt)\n",
    "\n",
    "En aquest darrer exercici, explorarem una característica molt interessant de Spark Streaming que et permet ***[unir dos fluxos](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins) i analitzar-los!***\n",
    "\n",
    "Per simplificar les coses, **ja et proporcionem dos fluxos de dades preagregades**. El primer, al tòpic Kafka **`mastodon_toots_original_domain`**, mostra el recompte de toots originals per diverses instàncies de Mastodon (recorda que Mastodon és una federació d'instàncies) en una **finestra fixa d'un minut**. El segon flux, al tòpic **`mastodon_toots_retoot_domain`**, presenta dades similars per als toots que són retoots d'altres toots. Les dades emmagatzemades als tòpics Kafka tenen la mateixa **estructura en format JSON**:\n",
    "- Una estructura `window` amb dos elements de tipus `string`: `start` i `end`\n",
    "- Un component `string` anomenat `mastodon_instance` amb el domini\n",
    "- Un element de tipus `integer` anomenat `count` amb el nombre de toots realitzats en aquell domini en la finestra de temps específica.\n",
    "\n",
    "Ja que l'estructura de les dades és bastant senzilla, **et demanem que la defineixis l'esquema manualment aquesta vegada**. Un cop hagis configurat les estructures, **hauràs d'obrir un flux per a cada origen Kafka**. El teu següent pas és unir aquests fluxos. Volem que facis un **left-join del flux de toots originals amb el flux de toots retuits**. Després de completar el join, la teva sortida **ha de incloure**:\n",
    "- **window:** l'interval de temps\n",
    "- **mastodon_instance:** el domini de la instància de Mastodon\n",
    "- **original_count:** nombre de toots originals publicats en aquell domini durant aquell interval de temps\n",
    "- **retweet_count:** nombre de toots retuits publicats en aquell domini durant aquell interval de temps\n",
    "\n",
    "**SUGGERIMENT:** Realitzar una unió en línia de dos fluxos no és una tasca fàcil, i hi ha moltes restriccions que has de respectar. **Llegeix [la documentació](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins) detingudament** i recorda que estem utilitzant la versió 2.4.0. A més, recorda que estem fent un **join en el temps, i això és una component clau**. Conceptes com els que has après sobre les finestres són fonamentals aquí, **juntament amb conceptes com [la marca d'aigua (watermarking)](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins)**. I recorda, **els modes de sortida**, has de triar-ne un que sigui adequat per a la tasca que vols fer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db86eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, to_timestamp, struct\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_4\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming data\n",
    "schema = StructType(<FILLIN>)\n",
    "\n",
    "# Define Kafka parameters\n",
    "toots_original_topic = <FILLIN>\n",
    "toots_retoot_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Create streaming DataFrame by reading original toots data from Kafka\n",
    "toots_original = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_original_df = toots_original\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Create streaming DataFrame by reading retoots data from Kafka\n",
    "toots_retoot = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_retoot_df = toots_retoot\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Join the two streams\n",
    "toots_join_df = toots_original_df.join(<FILLIN>...<FILLIN>)\n",
    "\n",
    "try:\n",
    "    # Start running the query that prints the running counts to the console\n",
    "    query = toots_join_df\\\n",
    "            .writeStream \\\n",
    "            <FILLIN>\n",
    "            ...\n",
    "            <FILLIN>\n",
    "            .option(\"numRows\", 100)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9d333",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "177aa2390a2ea73dd977e53f47950dc9",
     "grade": false,
     "grade_id": "video_response",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Video de Resposta (1 punt)\n",
    "\n",
    "En aquesta secció, hauràs de respondre a les següents preguntes en un vídeo de resposta **d'entre 3 i 6 minuts**. Has de fer una gravació en què el teu rostre sigui visible mentre respones a les preguntes amb les teves pròpies paraules. Has de seguir els punts següents per fer el vídeo (4 punts):\n",
    "\n",
    "1. Al principi del vídeo, **has de dir el teu nom i DNI mentre mostres el document** (que ha de ser clarament visible i llegible).\n",
    "2. Has de respondre les preguntes **en l'ordre que es donen**, comentant sempre a l'inici de cada resposta a quina pregunta et refereixes.\n",
    "3. En cas que no responguis a una pregunta, **has d'indicar el número de la pregunta que no estàs contestant i per què**.\n",
    "4. **Cada membre del grup ha de tenir una contribució comparable al vídeo**.\n",
    "\n",
    "### Preguntes:\n",
    "\n",
    "1. Durant els exercicis fets amb Spark Streaming, has utilitzat el paràmetre `group_id` per llegir les dades de Kafka. Explica què volen dir els grups a Kafka i per què necessites utilitzar aquest paràmetre per completar els exercicis.\n",
    "2. Has après com Spark Structured Streaming és eficaç per treballar amb dades estructurades. No obstant això, hi ha escenaris on una API de baix nivell com Spark Streaming pot ser més avantatjosa. **Proporciona un exemple utilitzant el mateix conjunt de dades d'aquesta activitat on Spark Streaming demostra ser més convenient que Structured Streaming**. Això ha de il·lustrar els avantatges de Spark Streaming en determinades situacions.\n",
    "3. Has practicat l'ús de **finestres per a agregacions basades en el temps**, però és important destacar que el mètode de definir aquestes finestres difereix entre les dues llibreries que hem estudiat. **Detalla aquestes diferències**, centrant-te en com cada llibreria descriu i implementa finestres temporals per a les agregacions.\n",
    "4. En aquest exercici, has estat mostrant els resultats a la consola, però Spark Structured Streaming ofereix una varietat d'altres formats de sortida. **Si us plau, descriu aquests formats**, destacant la gamma d'opcions disponibles per presentar i utilitzar els resultats generats per Spark Structured Streaming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
